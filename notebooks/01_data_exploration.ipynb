{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Docking Terminal: Exploratory Data Analysis\n",
    "\n",
    "This notebook explores the 60 simulation instances across 6 traffic levels to understand:\n",
    "- Traffic patterns and arrival distributions\n",
    "- Pallet flows by destination and type\n",
    "- Due date characteristics\n",
    "- Resource requirements and bottlenecks\n",
    "- Statistical summaries for all scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Dataset root not found: data",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m plt.rcParams[\u001b[33m'\u001b[39m\u001b[33mfont.size\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m10\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Initialize data loader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m config = load_config(\u001b[33m'\u001b[39m\u001b[33m../config/parameters.yaml\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEnvironment setup complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/truck-scheduling-model/notebooks/../src/data_loader.py:185\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset_root)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset_root = Path(dataset_root)\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset_root.exists():\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset root not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.dataset_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    187\u001b[39m \u001b[38;5;28mself\u001b[39m.scenarios = [\u001b[33m'\u001b[39m\u001b[33mHH_168h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMH_168h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMM_168h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLH_168h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLM_168h\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLL_168h\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    188\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader initialized with root: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.dataset_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Dataset root not found: data"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_loader import DataLoader, load_config\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Initialize data loader\n",
    "loader = DataLoader()\n",
    "config = load_config('../config/parameters.yaml')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Instances\n",
    "\n",
    "Load all 60 instances across the 6 traffic level scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all scenarios\n",
    "print(\"Loading all instances... (this may take a minute)\\n\")\n",
    "all_data = loader.load_all_scenarios()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n=== DATASET SUMMARY ===\")\n",
    "for scenario, instances in all_data.items():\n",
    "    print(f\"{scenario}: {len(instances)} instances loaded\")\n",
    "\n",
    "total_instances = sum(len(instances) for instances in all_data.values())\n",
    "print(f\"\\nTotal instances loaded: {total_instances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Comprehensive Instance Summary\n",
    "\n",
    "Generate a table with key statistics for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for all instances\n",
    "summary_df = loader.get_instance_summary()\n",
    "\n",
    "print(\"\\n=== INSTANCE SUMMARY (first 10 rows) ===\")\n",
    "print(summary_df.head(10))\n",
    "\n",
    "# Save to CSV\n",
    "output_path = Path('../results/tables/instance_summary.csv')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "summary_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nSummary saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traffic Level Comparison\n",
    "\n",
    "Compare average characteristics across different traffic levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scenario from instance name\n",
    "summary_df['scenario'] = summary_df['instance_name'].str.split('/').str[0]\n",
    "\n",
    "# Group by scenario\n",
    "scenario_stats = summary_df.groupby('scenario').agg({\n",
    "    'num_inbound_trucks': ['mean', 'std'],\n",
    "    'num_outbound_trucks': ['mean', 'std'],\n",
    "    'num_pallets': ['mean', 'std'],\n",
    "    'avg_pallet_due_date': 'mean',\n",
    "    'avg_outbound_due_date': 'mean',\n",
    "    'simulation_horizon': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n=== AVERAGE STATISTICS BY SCENARIO ===\")\n",
    "print(scenario_stats)\n",
    "\n",
    "# Save to CSV\n",
    "scenario_stats.to_csv('../results/tables/scenario_comparison.csv')\n",
    "print(\"\\nScenario comparison saved to: ../results/tables/scenario_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations: Traffic Patterns\n",
    "\n",
    "### 4.1 Number of Trucks by Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Inbound trucks\n",
    "summary_df.boxplot(column='num_inbound_trucks', by='scenario', ax=axes[0])\n",
    "axes[0].set_title('Inbound Trucks by Scenario')\n",
    "axes[0].set_xlabel('Scenario')\n",
    "axes[0].set_ylabel('Number of Inbound Trucks')\n",
    "axes[0].get_figure().suptitle('')  # Remove default title\n",
    "\n",
    "# Plot 2: Outbound trucks\n",
    "summary_df.boxplot(column='num_outbound_trucks', by='scenario', ax=axes[1])\n",
    "axes[1].set_title('Outbound Trucks by Scenario')\n",
    "axes[1].set_xlabel('Scenario')\n",
    "axes[1].set_ylabel('Number of Outbound Trucks')\n",
    "axes[1].get_figure().suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/trucks_by_scenario.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to: ../results/figures/trucks_by_scenario.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Number of Pallets by Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pallet counts by scenario\n",
    "plt.figure(figsize=(12, 6))\n",
    "summary_df.boxplot(column='num_pallets', by='scenario')\n",
    "plt.title('Pallet Count Distribution by Scenario')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Number of Pallets')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/pallets_by_scenario.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to: ../results/figures/pallets_by_scenario.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deep Dive: One Instance Analysis\n",
    "\n",
    "Detailed analysis of a single instance (MM_168h/instance1) to understand data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one instance for detailed analysis\n",
    "instance = loader.load_instance('MM_168h', 1)\n",
    "\n",
    "print(f\"\\n=== DETAILED ANALYSIS: {instance.instance_name} ===\")\n",
    "print(f\"\\nBasic Info:\")\n",
    "print(f\"  Inbound trucks: {len(instance.inbound_trucks)}\")\n",
    "print(f\"  Outbound trucks: {len(instance.outbound_trucks)}\")\n",
    "print(f\"  Pallets: {len(instance.pallets)}\")\n",
    "\n",
    "# Convert to DataFrames for analysis\n",
    "inbound_df = pd.DataFrame([\n",
    "    {'truck_id': t.truck_id, 'arrival_time': t.arrival_time}\n",
    "    for t in instance.inbound_trucks\n",
    "])\n",
    "\n",
    "outbound_df = pd.DataFrame([\n",
    "    {'truck_id': t.truck_id, 'arrival_time': t.arrival_time, \n",
    "     'due_date': t.due_date, 'destination': t.destination}\n",
    "    for t in instance.outbound_trucks\n",
    "])\n",
    "\n",
    "pallet_df = pd.DataFrame([\n",
    "    {'pallet_id': p.pallet_id, 'due_date': p.due_date,\n",
    "     'destination': p.destination, 'type': p.pallet_type,\n",
    "     'inbound_truck': p.inbound_truck_id}\n",
    "    for p in instance.pallets\n",
    "])\n",
    "\n",
    "print(f\"\\nPallet Distribution:\")\n",
    "print(f\"  By Destination: {pallet_df['destination'].value_counts().to_dict()}\")\n",
    "print(f\"  By Type: {pallet_df['type'].value_counts().to_dict()}\")\n",
    "print(f\"\\nOutbound Truck Distribution:\")\n",
    "print(f\"  By Destination: {outbound_df['destination'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Arrival Time Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot arrival time distributions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Inbound truck arrivals\n",
    "axes[0].hist(inbound_df['arrival_time'], bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Inbound Truck Arrival Distribution (MM_168h/instance1)')\n",
    "axes[0].set_xlabel('Arrival Time (minutes)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Outbound truck arrivals\n",
    "axes[1].hist(outbound_df['arrival_time'], bins=50, alpha=0.7, color='coral', edgecolor='black')\n",
    "axes[1].set_title('Outbound Truck Arrival Distribution (MM_168h/instance1)')\n",
    "axes[1].set_xlabel('Arrival Time (minutes)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/arrival_distributions_MM_instance1.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to: ../results/figures/arrival_distributions_MM_instance1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Due Date Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot due date distributions\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Pallet due dates\n",
    "axes[0].hist(pallet_df['due_date'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0].set_title('Pallet Due Date Distribution (MM_168h/instance1)')\n",
    "axes[0].set_xlabel('Due Date (minutes)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Outbound truck due dates\n",
    "axes[1].hist(outbound_df['due_date'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1].set_title('Outbound Truck Due Date Distribution (MM_168h/instance1)')\n",
    "axes[1].set_xlabel('Due Date (minutes)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/due_date_distributions_MM_instance1.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to: ../results/figures/due_date_distributions_MM_instance1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Pallet Flow by Destination and Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-tabulation\n",
    "pallet_crosstab = pd.crosstab(pallet_df['destination'], pallet_df['type'])\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart\n",
    "pallet_crosstab.plot(kind='bar', stacked=True, ax=axes[0], color=['#66c2a5', '#fc8d62', '#8da0cb'])\n",
    "axes[0].set_title('Pallets by Destination and Type (Stacked)')\n",
    "axes[0].set_xlabel('Destination')\n",
    "axes[0].set_ylabel('Number of Pallets')\n",
    "axes[0].legend(title='Pallet Type')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Grouped bar chart\n",
    "pallet_crosstab.plot(kind='bar', ax=axes[1], color=['#66c2a5', '#fc8d62', '#8da0cb'])\n",
    "axes[1].set_title('Pallets by Destination and Type (Grouped)')\n",
    "axes[1].set_xlabel('Destination')\n",
    "axes[1].set_ylabel('Number of Pallets')\n",
    "axes[1].legend(title='Pallet Type')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/pallet_flow_analysis_MM_instance1.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCross-tabulation:\")\n",
    "print(pallet_crosstab)\n",
    "print(\"\\nFigure saved to: ../results/figures/pallet_flow_analysis_MM_instance1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Time Window Analysis (Slack Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate slack time (due_date - arrival_time) for outbound trucks\n",
    "outbound_df['slack_time'] = outbound_df['due_date'] - outbound_df['arrival_time']\n",
    "\n",
    "# Visualize slack time distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(outbound_df['slack_time'], bins=30, alpha=0.7, color='teal', edgecolor='black')\n",
    "plt.axvline(outbound_df['slack_time'].mean(), color='red', linestyle='--', \n",
    "            linewidth=2, label=f\"Mean: {outbound_df['slack_time'].mean():.1f} min\")\n",
    "plt.axvline(outbound_df['slack_time'].median(), color='orange', linestyle='--', \n",
    "            linewidth=2, label=f\"Median: {outbound_df['slack_time'].median():.1f} min\")\n",
    "plt.title('Outbound Truck Slack Time Distribution (Due Date - Arrival Time)')\n",
    "plt.xlabel('Slack Time (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/slack_time_distribution_MM_instance1.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSlack Time Statistics:\")\n",
    "print(f\"  Mean: {outbound_df['slack_time'].mean():.2f} minutes\")\n",
    "print(f\"  Median: {outbound_df['slack_time'].median():.2f} minutes\")\n",
    "print(f\"  Std Dev: {outbound_df['slack_time'].std():.2f} minutes\")\n",
    "print(f\"  Min: {outbound_df['slack_time'].min():.2f} minutes\")\n",
    "print(f\"  Max: {outbound_df['slack_time'].max():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resource Requirement Analysis\n",
    "\n",
    "Estimate peak resource utilization across all scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate peak arrival rates\n",
    "def calculate_peak_arrivals(instance, window_hours=1):\n",
    "    \"\"\"Calculate peak arrival rates in a sliding time window.\"\"\"\n",
    "    window_minutes = window_hours * 60\n",
    "    \n",
    "    # Get all inbound arrivals\n",
    "    arrivals = sorted([t.arrival_time for t in instance.inbound_trucks])\n",
    "    max_time = max(arrivals)\n",
    "    \n",
    "    # Sliding window to find peak\n",
    "    max_count = 0\n",
    "    for start_time in range(0, int(max_time), 30):  # Check every 30 minutes\n",
    "        end_time = start_time + window_minutes\n",
    "        count = sum(1 for a in arrivals if start_time <= a < end_time)\n",
    "        max_count = max(max_count, count)\n",
    "    \n",
    "    return max_count\n",
    "\n",
    "# Calculate for each scenario\n",
    "resource_analysis = []\n",
    "for scenario, instances in all_data.items():\n",
    "    for instance in instances:\n",
    "        peak_1h = calculate_peak_arrivals(instance, window_hours=1)\n",
    "        resource_analysis.append({\n",
    "            'scenario': scenario,\n",
    "            'instance': instance.instance_name,\n",
    "            'peak_arrivals_1h': peak_1h,\n",
    "            'num_pallets': len(instance.pallets),\n",
    "            'num_outbound_trucks': len(instance.outbound_trucks)\n",
    "        })\n",
    "\n",
    "resource_df = pd.DataFrame(resource_analysis)\n",
    "\n",
    "# Group by scenario\n",
    "peak_by_scenario = resource_df.groupby('scenario')['peak_arrivals_1h'].agg(['mean', 'max']).round(1)\n",
    "\n",
    "print(\"\\n=== PEAK INBOUND ARRIVAL RATES (per hour window) ===\")\n",
    "print(peak_by_scenario)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "resource_df.boxplot(column='peak_arrivals_1h', by='scenario')\n",
    "plt.title('Peak Inbound Truck Arrivals (1-hour window) by Scenario')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Peak Arrivals per Hour')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/peak_arrivals_by_scenario.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved to: ../results/figures/peak_arrivals_by_scenario.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics Export\n",
    "\n",
    "Generate comprehensive summary tables for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed statistics table\n",
    "detailed_stats = []\n",
    "\n",
    "for scenario, instances in all_data.items():\n",
    "    for instance in instances:\n",
    "        # Convert pallets to DataFrame\n",
    "        pallet_df_temp = pd.DataFrame([\n",
    "            {'due_date': p.due_date, 'destination': p.destination, 'type': p.pallet_type}\n",
    "            for p in instance.pallets\n",
    "        ])\n",
    "        \n",
    "        outbound_df_temp = pd.DataFrame([\n",
    "            {'arrival': t.arrival_time, 'due_date': t.due_date, 'destination': t.destination}\n",
    "            for t in instance.outbound_trucks\n",
    "        ])\n",
    "        \n",
    "        stats = {\n",
    "            'scenario': scenario,\n",
    "            'instance': instance.instance_name,\n",
    "            'num_inbound': len(instance.inbound_trucks),\n",
    "            'num_outbound': len(instance.outbound_trucks),\n",
    "            'num_pallets': len(instance.pallets),\n",
    "            'pallets_dest1': (pallet_df_temp['destination'] == 1).sum(),\n",
    "            'pallets_dest2': (pallet_df_temp['destination'] == 2).sum(),\n",
    "            'pallets_dest3': (pallet_df_temp['destination'] == 3).sum(),\n",
    "            'pallets_typeA': (pallet_df_temp['type'] == 'A').sum(),\n",
    "            'pallets_typeB': (pallet_df_temp['type'] == 'B').sum(),\n",
    "            'pallets_typeC': (pallet_df_temp['type'] == 'C').sum(),\n",
    "            'avg_pallet_due_date': pallet_df_temp['due_date'].mean(),\n",
    "            'avg_truck_slack': (outbound_df_temp['due_date'] - outbound_df_temp['arrival']).mean(),\n",
    "        }\n",
    "        detailed_stats.append(stats)\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_stats)\n",
    "\n",
    "# Save to CSV\n",
    "detailed_df.to_csv('../results/tables/detailed_instance_statistics.csv', index=False)\n",
    "print(\"\\nDetailed statistics saved to: ../results/tables/detailed_instance_statistics.csv\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(detailed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Recommendations\n",
    "\n",
    "Based on the exploratory analysis, here are the key findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TRAFFIC LEVEL CHARACTERISTICS:\")\n",
    "for scenario in ['HH_168h', 'MH_168h', 'MM_168h', 'LH_168h', 'LM_168h', 'LL_168h']:\n",
    "    scenario_data = [inst for inst in all_data[scenario]]\n",
    "    if scenario_data:\n",
    "        avg_pallets = np.mean([len(inst.pallets) for inst in scenario_data])\n",
    "        print(f\"   {scenario}: ~{avg_pallets:.0f} pallets on average\")\n",
    "\n",
    "print(\"\\n2. PALLET DISTRIBUTION:\")\n",
    "print(\"   - Pallets are roughly equally distributed across 3 destinations\")\n",
    "print(\"   - Pallet types (A, B, C) are roughly equally distributed\")\n",
    "\n",
    "print(\"\\n3. RESOURCE REQUIREMENTS:\")\n",
    "print(f\"   - Current configuration: {config['facility']['num_forklifts']} forklifts\")\n",
    "print(f\"   - Truck capacity: {config['trucks']['outbound_truck_capacity']} pallets\")\n",
    "print(\"   - Peak arrival rates vary significantly by scenario (see figures)\")\n",
    "\n",
    "print(\"\\n4. OPTIMIZATION CHALLENGES:\")\n",
    "print(\"   - Time window constraints from due dates\")\n",
    "print(\"   - Matching pallets to correct destination trucks\")\n",
    "print(\"   - Maximizing truck fill rates while meeting deadlines\")\n",
    "print(\"   - Limited dock doors (1 inbound, 1 outbound)\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Implement MILP models for truck scheduling and pallet assignment\")\n",
    "print(\"   - Develop heuristic algorithms for real-time decision making\")\n",
    "print(\"   - Build simulation framework to validate optimization solutions\")\n",
    "print(\"   - Benchmark performance across all 60 instances\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nAll figures and tables saved to ../results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exploratory analysis provides a solid foundation for building optimization models and simulation frameworks. The data is well-structured, clean (after handling missing values), and ready for advanced modeling.\n",
    "\n",
    "**Next notebook**: `02_optimization_experiments.ipynb` - Implementing MILP models and heuristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
